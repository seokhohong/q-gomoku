# //////////////////////////////////////////////////////////////////////////////
# ATTENTION
# //////////////////////////////////////////////////////////////////////////////
#
# This file is managed through git and is symlinked from a cloned repository
# on this machine. Type `ls -al` to view the source location of this file.
# Any changes made must be checked in to git otherwise they will be overwritten.
# https://wwwin-github.cisco.com/GVS-CS-DSX/server_config
#
# //////////////////////////////////////////////////////////////////////////////

spark.master            yarn
spark.deploy-mode       cluster
spark.driver.memory     14440M
spark.ui.port           4080

spark.dynamicAllocation.enabled                             true
spark.dynamicAllocation.schedulerBacklogTimeout             1s
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout    1s
spark.dynamicAllocation.executorIdleTimeout                 300s
spark.shuffle.service.enabled                               true

spark.shuffle.consolidateFiles    true
spark.io.compression.codec        org.apache.spark.io.LZ4CompressionCodec

spark.sql.warehouse.dir           maprfs:/app/GCS_ANA/hive/warehouse
spark.local.dir                   /hdfs/app/GCS_ANA/tmp
spark.locality.wait               1

spark.driver.maxResultSize        8g
spark.debug.maxToStringFields     9999999
spark.rpc.message.maxSize         256

spark.network.timeout             1600

spark.submit.deployMode           client

spark.sql.hive.convertMetastoreParquet                  false

# Hadoop settings
# Prefix with 'spark.hadoop' and they will be passed to the hadoop config
spark.hadoop.mapred.input.dir.recursive                              true
spark.hadoop.hive.mapred.supports.subdirectories                     true
spark.hadoop.mapreduce.input.fileinputformat.input.dir.recursive     true

# ==============================================================================
# Google Cloud Storage Setup
# ==============================================================================
# Required. Google Cloud Project ID with access to configured GCS buckets.
spark.hadoop.fs.gs.project.id                 gvs-cs-cisco

# Deprecated. GCS bucket to use as a default bucket if fs.default.name
# is not a gs: uri.
# spark.hadoop.fs.gs.system.bucket

# The directory relative gs: uris resolve in inside of the default bucket.
spark.hadoop.fs.gs.working.dir                /hadoop/warehouse

# The FileSystem for gs: (GCS) uris.
spark.hadoop.fs.gs.impl                       com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem

# The AbstractFileSystem for gs: (GCS) uris.
spark.hadoop.fs.AbstractFileSystem.gs.impl    com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS

# Whether or not to create an in memory cache of GCS objects created by
# the connector. This allows for immediate visibility of objects to
# subsequent list requests from the same client.
spark.hadoop.fs.gs.metadata.cache.enable      true

# Whether or not to create objects for the parent directories of objects
# with / in their path e.g. creating gs://bucket/foo/ upon finding
# gs://bucket/foo/bar.
spark.hadoop.fs.gs.implicit.directory.repair  true

# Whether or not to prepopulate potenital glob matches in a single list
# request to minimize calls to GCS in nested glob cases.
spark.hadoop.fs.gs.glob.flatlist.enable       true

# ==============================================================================
# Google Cloud Storage Authorization
# ==============================================================================
# Whether to use a service account for GCS authorizaiton. If an email and
# keyfile are provided (see google.cloud.auth.service.account.email and
# google.cloud.auth.service.account.keyfile), then that service account
# will be used. Otherwise the connector will look to see if it running on
# a GCE VM with some level of GCS access in it's service account scope, and
# use that service account.
spark.hadoop.google.cloud.auth.service.account.enable         true

# The JSON key file of the service account used for GCS
# access when google.cloud.auth.service.account.enable is true.
spark.hadoop.google.cloud.auth.service.account.json.keyfile   /hdfs/app/GCS_ANA/google/keys/hdprd-edge@gvs-cs-cisco.iam.gserviceaccount.com.json

# ==============================================================================
# Google Cloud Storage Block, Buffer, and file sizes
# ==============================================================================
# The reported block size of the file system. This does not change any
# behavior of the connector or the underlying GCS objectsr. However it
# will affect the number of splits Hadoop MapReduce uses for a given
# input.
spark.hadoop.fs.gs.block.size           67108864

# The number of bytes in read buffers.
spark.hadoop.fs.gs.io.buffersize        8388608

# The number of bytes in write buffers.
spark.hadoop.fs.gs.io.buffersize.write  67108864
