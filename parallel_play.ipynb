{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Input, Convolution2D, Dense, Dropout, Flatten, concatenate, BatchNormalization\n",
    "from keras.models import Model  # basic class for specifying and training a neural network\n",
    "from keras import losses\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "random_state = np.random.RandomState(42)\n",
    "\n",
    "MIN_Q = -1\n",
    "MAX_Q = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Move:\n",
    "    def __init__(self, player, x, y):\n",
    "        self.player = player\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "def peek_stack(list):\n",
    "    if len(list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return list[-1]\n",
    "    \n",
    "class GameState():\n",
    "    WON = 1\n",
    "    DRAW = 2\n",
    "    NOT_OVER = 3\n",
    "\n",
    "class Board:\n",
    "    player_to_move: int\n",
    "    NO_PLAYER = 0\n",
    "    FIRST_PLAYER = 1\n",
    "    SECOND_PLAYER = -1\n",
    "\n",
    "    def __init__(self, size=5, win_chain_length=3):\n",
    "        self.size = size\n",
    "        self.matrix = np.zeros((self.size, self.size))\n",
    "        self.matrix.fill(Board.NO_PLAYER)\n",
    "        self.win_chain_length = win_chain_length\n",
    "        # store (x, y, player) tuple\n",
    "        self.ops = []\n",
    "        self.player_to_move = 1\n",
    "        self.game_state = GameState.NOT_OVER\n",
    "        self.available_moves = set()\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                self.available_moves.add((i, j))\n",
    "\n",
    "        self.cached_point_rotations = defaultdict(list)\n",
    "        self.cache_rotations()\n",
    "\n",
    "        # whether current game_state is accurate\n",
    "        self.state_computed = False\n",
    "\n",
    "    def _mark_not_computed(self):\n",
    "        self.state_computed = False\n",
    "\n",
    "    # lightweight version of move\n",
    "    def hypothetical_move(self, x, y):\n",
    "        assert (self.game_state is GameState.NOT_OVER)\n",
    "        self.ops.append(Move(self.player_to_move, x, y))\n",
    "        self.matrix[x, y] = self.player_to_move\n",
    "        self.available_moves.remove((x, y))\n",
    "        self.flip_player_to_move()\n",
    "        self._mark_not_computed()\n",
    "\n",
    "    def unmove(self):\n",
    "        previous_move = self.ops.pop()\n",
    "        self.matrix[previous_move.x, previous_move.y] = Board.NO_PLAYER\n",
    "        self.available_moves.add((previous_move.x, previous_move.y))\n",
    "        self.flip_player_to_move()\n",
    "        self.game_state = GameState.NOT_OVER\n",
    "\n",
    "    # +1 for self, -1 for other\n",
    "    def get_matrix(self, as_player):\n",
    "        if as_player == Board.FIRST_PLAYER:\n",
    "            return np.copy(self.matrix)\n",
    "        return -np.copy(self.matrix)\n",
    "\n",
    "    def get_rotated_matrices(self, as_player):\n",
    "        matrix = self.get_matrix(as_player)\n",
    "        return [\n",
    "            matrix,\n",
    "            matrix.transpose(),\n",
    "            np.rot90(matrix),\n",
    "            np.rot90(matrix).transpose(),\n",
    "            np.rot90(matrix, 2),\n",
    "            np.rot90(matrix, 2).transpose(),\n",
    "            np.rot90(matrix, 3),\n",
    "            np.rot90(matrix, 3).transpose()\n",
    "        ]\n",
    "\n",
    "    def cache_rotations(self):\n",
    "        indices = np.array(range(self.size ** 2)).reshape(self.size, self.size)\n",
    "        for matrix in [\n",
    "                indices,\n",
    "                indices.transpose(),\n",
    "                np.rot90(indices),\n",
    "                np.rot90(indices).transpose(),\n",
    "                np.rot90(indices, 2),\n",
    "                np.rot90(indices, 2).transpose(),\n",
    "                np.rot90(indices, 3),\n",
    "                np.rot90(indices, 3).transpose()\n",
    "            ]:\n",
    "            for x in range(self.size):\n",
    "                for y in range(self.size):\n",
    "                    self.cached_point_rotations[matrix[x, y]].append(indices[x, y])\n",
    "\n",
    "\n",
    "    def coordinate_to_index(self, x, y):\n",
    "        return x * self.size + y\n",
    "\n",
    "    def get_rotated_point(self, index):\n",
    "        return self.cached_point_rotations[index]\n",
    "\n",
    "    # deprecate\n",
    "    def make_move(self, x, y):\n",
    "        assert(self.game_state is GameState.NOT_OVER)\n",
    "        self.ops.append(Move(self.player_to_move, x, y))\n",
    "        self.matrix[x, y] = self.player_to_move\n",
    "        self.available_moves.remove((x, y))\n",
    "        self.flip_player_to_move()\n",
    "        self.compute_game_state()\n",
    "\n",
    "    def make_random_move(self):\n",
    "        move_x, move_y = random.choice(list(self.available_moves))\n",
    "        self.hypothetical_move(move_x, move_y)\n",
    "\n",
    "    def flip_player_to_move(self):\n",
    "        if self.player_to_move == Board.FIRST_PLAYER:\n",
    "            self.player_to_move = Board.SECOND_PLAYER\n",
    "        else:\n",
    "            self.player_to_move = Board.FIRST_PLAYER\n",
    "\n",
    "    # returns None if game has not concluded, True if the last move won the game, False if draw\n",
    "    # frequently called function, needs to be optimized\n",
    "    def compute_game_state(self):\n",
    "        last_move = peek_stack(self.ops)\n",
    "        if last_move:\n",
    "            last_x, last_y = last_move.x, last_move.y\n",
    "            if self.chain_length(last_x, last_y, -1, 0) + self.chain_length(last_x, last_y, 1, 0) >= self.win_chain_length + 1\\\n",
    "                    or self.chain_length(last_x, last_y, -1, 1) + self.chain_length(last_x, last_y, 1, -1) >= self.win_chain_length + 1\\\n",
    "                    or self.chain_length(last_x, last_y, 1, 1) + self.chain_length(last_x, last_y, -1, -1) >= self.win_chain_length + 1\\\n",
    "                    or self.chain_length(last_x, last_y, 0, 1) + self.chain_length(last_x, last_y, 0, -1) >= self.win_chain_length + 1:\n",
    "                self.game_state = GameState.WON\n",
    "                return\n",
    "            if len(self.ops) == self.size ** 2:\n",
    "                self.game_state = GameState.DRAW\n",
    "                return\n",
    "        self.game_state = GameState.NOT_OVER\n",
    "        self.state_computed = True\n",
    "\n",
    "    def in_bounds(self, x, y):\n",
    "        return 0 <= x < self.size and 0 <= y < self.size\n",
    "\n",
    "    def chain_length(self, center_x, center_y, delta_x, delta_y):\n",
    "        center_stone = self.matrix[center_x, center_y]\n",
    "        if center_stone == Board.NO_PLAYER:\n",
    "            return 0\n",
    "        chain_length = 1\n",
    "        for step in range(1, self.win_chain_length):\n",
    "            step_x = delta_x * step\n",
    "            step_y = delta_y * step\n",
    "            if 0 <= center_x + step_x < self.size and 0 <= center_y + step_y < self.size and \\\n",
    "                    self.matrix[center_x + step_x, center_y + step_y] == center_stone:\n",
    "                chain_length += 1\n",
    "            else:\n",
    "                break\n",
    "        return chain_length\n",
    "\n",
    "    # runs a full compute\n",
    "    def game_won(self):\n",
    "        if not self.state_computed:\n",
    "            self.compute_game_state()\n",
    "        return self.game_state == GameState.WON\n",
    "\n",
    "    # probably drawn, cheap check\n",
    "    def game_drawn(self):\n",
    "        return len(self.ops) == self.size ** 2\n",
    "\n",
    "    def game_over(self):\n",
    "        if not self.state_computed:\n",
    "            self.compute_game_state()\n",
    "        return self.game_state != GameState.NOT_OVER\n",
    "\n",
    "    def pprint(self):\n",
    "        def display_char(x, y):\n",
    "            move = peek_stack(self.ops)\n",
    "            if move:\n",
    "                was_last_move = (x == move.x and y == move.y)\n",
    "                if self.matrix[x, y] == Board.FIRST_PLAYER:\n",
    "                    if was_last_move:\n",
    "                        return 'X'\n",
    "                    return 'x'\n",
    "                elif self.matrix[x, y] == Board.SECOND_PLAYER:\n",
    "                    if was_last_move:\n",
    "                        return 'O'\n",
    "                    return 'o'\n",
    "            return ' '\n",
    "        board_string = \"\"\n",
    "        for i in range(0, self.size):\n",
    "            board_string += \"\\n\"\n",
    "            for j in range(self.size):\n",
    "                board_string += \"|\" + display_char(j, i)\n",
    "            board_string += \"|\"\n",
    "        return board_string\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.pprint()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MoveList:\n",
    "    # moves should be a tuple\n",
    "    def __init__(self, moves):\n",
    "        self.moves = moves\n",
    "        # for move in moves:\n",
    "        #    assert(len(move) == 2)\n",
    "\n",
    "    def append(self, new_move):\n",
    "        return MoveList((self.moves + (new_move,)))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.moves == other.moves\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.moves)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.moves)\n",
    "\n",
    "# principle value search\n",
    "class PVSNode:\n",
    "    MAX_Q = 1\n",
    "    MIN_Q = -1\n",
    "\n",
    "    def __init__(self, parent, is_maximizing, full_move_list):\n",
    "\n",
    "        # parent TreeNode\n",
    "        self.parent = parent\n",
    "        # full move tree\n",
    "        self.full_move_list = full_move_list\n",
    "\n",
    "        self.is_maximizing = is_maximizing\n",
    "        # is the game over with this move list\n",
    "        self.game_status = GameState.NOT_OVER\n",
    "        # for debugging\n",
    "        # key is move\n",
    "        self.children = {}\n",
    "\n",
    "        # tuple of PVSNode, q\n",
    "        self.principle_variation = None\n",
    "        self.q = None\n",
    "\n",
    "        # log likelihood of playing this move given parent state\n",
    "        self.log_local_p = 0\n",
    "        # log likelihood of playing this move given root board state (used for PVS search)\n",
    "        self.log_total_p = 0\n",
    "\n",
    "        # the quality of this move\n",
    "        self.move_goodness = 0\n",
    "\n",
    "    def has_children(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    # note this does NOT compute q for child\n",
    "    def create_child(self, move):\n",
    "        child = PVSNode(parent=self,\n",
    "                        is_maximizing=not self.is_maximizing,\n",
    "                        full_move_list=self.full_move_list.append(move))\n",
    "        self.children[move] = child\n",
    "        return child\n",
    "\n",
    "    def get_sorted_moves(self):\n",
    "        return sorted(self.children.items(), key=lambda x: x[1].move_goodness, reverse=self.is_maximizing)\n",
    "\n",
    "    # used ONLY for leaf assignment\n",
    "    def assign_q(self, q, game_status):\n",
    "        assert (len(self.children) == 0)\n",
    "\n",
    "        # mark game status\n",
    "        self.game_status = game_status\n",
    "\n",
    "        # for a leaf node, the principle variation is itself\n",
    "        self.principle_variation = self\n",
    "        self.q = q\n",
    "\n",
    "        self.assign_move_goodness()\n",
    "\n",
    "    def assign_move_goodness(self):\n",
    "        # play shorter sequences if advantageous, otherwise play longer sequences\n",
    "        if self.q > 0:\n",
    "            self.move_goodness = self.q - len(self.principle_variation.full_move_list) * 0.001\n",
    "        else:\n",
    "            self.move_goodness = self.q + len(self.principle_variation.full_move_list) * 0.001\n",
    "\n",
    "    def assign_p(self, log_p):\n",
    "        self.log_local_p = log_p\n",
    "        self.log_total_p = self.parent.log_total_p + self.log_local_p\n",
    "\n",
    "    def recalculate_q(self):\n",
    "        # take the largest (or smallest) q across all seen moves\n",
    "\n",
    "        # if this node is still a leaf, break\n",
    "        if len(self.children) == 0:\n",
    "            return\n",
    "\n",
    "        if self.principle_variation:\n",
    "            prev_q = self.q\n",
    "        else:\n",
    "            prev_q = np.inf\n",
    "\n",
    "        if self.is_maximizing:\n",
    "            best_move = max(self.children.items(), key=lambda x : x[1].move_goodness)[0]\n",
    "        else:\n",
    "            best_move = min(self.children.items(), key=lambda x : x[1].move_goodness)[0]\n",
    "\n",
    "        # using negamax framework\n",
    "        self.principle_variation = self.children[best_move].principle_variation\n",
    "        self.q = self.children[best_move].q\n",
    "\n",
    "        self.assign_move_goodness()\n",
    "\n",
    "        if self.parent and abs(prev_q - self.q) > 1E-6:\n",
    "            # update pvs for parent\n",
    "            self.parent.recalculate_q()\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.principle_variation:\n",
    "            return (\"PV: \" + str(self.principle_variation.full_move_list.moves) + \" Q: {0:.4f} P: {1:.4f}\").format(self.q, self.log_total_p)\n",
    "        else:\n",
    "            return \"Position: \" + str(self.full_move_list.moves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PQMind:\n",
    "    def __init__(self, size, alpha, turn_input=True, init=True):\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "        self.value_est = self.get_value_model()\n",
    "        self.policy_est = self.get_policy_model()\n",
    "\n",
    "        # initialization\n",
    "        init_examples = 10\n",
    "\n",
    "        if init:\n",
    "            sample_x = [\n",
    "                                    random_state.random_integers(-1, 1, size=(init_examples, size, size, 1)),\n",
    "                                    random_state.random_integers(-1, 1, size=(init_examples)).reshape(init_examples, -1),\n",
    "                                ]\n",
    "            self.value_est.fit(sample_x, y=np.zeros((init_examples)), epochs=1, batch_size=100)\n",
    "            self.policy_est.fit(sample_x, y=np.zeros((init_examples, self.size ** 2)))\n",
    "\n",
    "        self.train_vectors = []\n",
    "        self.train_q = []\n",
    "        self.train_p = []\n",
    "\n",
    "        self.fitted = False\n",
    "\n",
    "        self.turn_input = turn_input\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def get_layers(self):\n",
    "        inp = Input(shape=(self.size, self.size, 1))\n",
    "\n",
    "        #bn1 = BatchNormalization()(inp)\n",
    "        # key difference between this and conv network is padding\n",
    "        conv_1 = Convolution2D(64, (3, 3), padding='same', activation='relu',\n",
    "                               kernel_initializer='random_uniform')(inp)\n",
    "        bn2 = BatchNormalization()(conv_1)\n",
    "        conv_2 = Convolution2D(64, (3, 3), padding='same', activation='relu',\n",
    "                               kernel_initializer='random_uniform')(bn2)\n",
    "        bn3 = BatchNormalization()(conv_2)\n",
    "        conv_3 = Convolution2D(64, (3, 3), padding='valid', activation='relu',\n",
    "                               kernel_initializer='random_uniform')(bn3)\n",
    "        bn4 = BatchNormalization()(conv_3)\n",
    "\n",
    "        flat = Flatten()(bn4)\n",
    "        turn_input = Input(shape=(1,), name='turn')\n",
    "        full = concatenate([flat, turn_input])\n",
    "\n",
    "        hidden = Dense(30, activation='relu', kernel_initializer='random_uniform')(full)\n",
    "        #bn4 = BatchNormalization()(hidden)\n",
    "\n",
    "        return inp, turn_input, hidden\n",
    "\n",
    "    def get_value_model(self):\n",
    "        inp, turn_input, hidden = self.get_layers()\n",
    "\n",
    "        out = Dense(1)(hidden)\n",
    "\n",
    "        model = Model(inputs=[inp, turn_input], outputs=out)\n",
    "        model.compile(loss=losses.mean_squared_error, optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_policy_model(self):\n",
    "        inp, turn_input, hidden = self.get_layers()\n",
    "\n",
    "\n",
    "        out = Dense(self.size ** 2, activation='softmax')(hidden)\n",
    "\n",
    "        model = Model(inputs=[inp, turn_input], outputs=out)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    # after parents are expanded, this method recomputes all leaves\n",
    "    def pvs_catch_leaves(self, leaf_nodes, new_parents, max_depth=5):\n",
    "        for parent in new_parents:\n",
    "            if parent in leaf_nodes:\n",
    "                leaf_nodes.remove(parent)\n",
    "            if len(parent.full_move_list) < max_depth:\n",
    "                leaf_nodes.update([node for node in parent.children.values() if node.game_status == GameState.NOT_OVER])\n",
    "\n",
    "    def pvs_k_principle_variations(self, root_node, leaf_nodes, k=5):\n",
    "        # include the best move according to q\n",
    "        q_best = root_node.principle_variation\n",
    "        # draw\n",
    "        if q_best is None:\n",
    "            return None\n",
    "\n",
    "        # include k-1 most likely moves according to p\n",
    "        if q_best.game_status == GameState.NOT_OVER:\n",
    "            return [q_best] + list(leaf_nodes.islice(0, k - 2))\n",
    "        else:\n",
    "            return leaf_nodes.islice(0, k - 1)\n",
    "\n",
    "    # board perception AND move turn perception will always be from the perspective of Player 1\n",
    "    # Q will always be from the perspective of Player 1 (Player 1 Wins = Q = 1, Player -1 Wins, Q = -1)\n",
    "\n",
    "    def pvs_best_moves(self, board, max_iters=10, epsilon=0.01, verbose=True, k=25, max_depth=5):\n",
    "        root_node = PVSNode(parent=None,\n",
    "                                    is_maximizing=True if board.player_to_move == 1 else False,\n",
    "                                    full_move_list=MoveList(moves=()))\n",
    "\n",
    "        principle_variations = [root_node]\n",
    "        leaf_nodes = SortedSet(principle_variations, key=lambda x: x.log_total_p)\n",
    "\n",
    "        for i in range(max_iters):\n",
    "            self.pvs_batch(board, principle_variations)\n",
    "            self.pvs_catch_leaves(leaf_nodes, principle_variations, max_depth=max_depth)\n",
    "            principle_variations = self.pvs_k_principle_variations(root_node, leaf_nodes, k=k)\n",
    "\n",
    "            if not principle_variations:\n",
    "                print(\"Exhausted Search\")\n",
    "                break\n",
    "\n",
    "        # find best node (highest q)\n",
    "        possible_moves = root_node.get_sorted_moves()\n",
    "\n",
    "        for move, q in possible_moves:\n",
    "            node = root_node.children[move]\n",
    "            print(str(node.principle_variation))\n",
    "\n",
    "        return possible_moves\n",
    "\n",
    "    def pvs(self, board, max_iters=10, epsilon=0.01, verbose=True, max_depth=5, k=25):\n",
    "\n",
    "        # array of [best_move, best_node]\n",
    "        possible_moves = self.pvs_best_moves(board,\n",
    "                                             max_iters=max_iters,\n",
    "                                             epsilon=epsilon,\n",
    "                                             verbose=verbose,\n",
    "                                             k=k,\n",
    "                                             max_depth=max_depth)\n",
    "\n",
    "        # best action is 0th index\n",
    "        picked_action = 0\n",
    "\n",
    "        # pick a suboptimal move\n",
    "        if random_state.random_sample() < epsilon:\n",
    "            if verbose:\n",
    "                print('suboptimal move')\n",
    "            # abs is only there to handle floating point problems\n",
    "            qs = np.array([node.q for _, node in possible_moves])\n",
    "            if board.player_to_move == 1:\n",
    "                distribution = np.abs(qs + 1.0) / 2\n",
    "            else:\n",
    "                # not sure this is correct\n",
    "                distribution = sorted(-np.abs(qs - 1.0) / 2)\n",
    "\n",
    "            if sum(distribution) > 0:\n",
    "                distribution = (distribution.astype(np.float64) / sum(distribution))\n",
    "                picked_action = np.random.choice(range(len(possible_moves)), 1, p=distribution)[0]\n",
    "\n",
    "        best_move, best_node = possible_moves[picked_action]\n",
    "\n",
    "        return best_move, best_node.q\n",
    "\n",
    "    def pvs_vectors(self, board, nodes_to_expand):\n",
    "        # each board state is defined by a list of moves\n",
    "        q_search_nodes = []\n",
    "        q_search_vectors = []\n",
    "        q_search_player = []\n",
    "\n",
    "        p_search_vectors = []\n",
    "        p_search_players = []\n",
    "\n",
    "        validation_matrix = np.copy(board.matrix)\n",
    "\n",
    "        for parent in nodes_to_expand:\n",
    "            # for each move except the last, make rapid moves on board\n",
    "            for move in parent.full_move_list.moves:\n",
    "                board.hypothetical_move(move[0], move[1])\n",
    "\n",
    "            for child_move in copy.copy(board.available_moves):\n",
    "                child = parent.create_child(child_move)\n",
    "                board.hypothetical_move(child_move[0], child_move[1])\n",
    "                # if game is over, then we have our q\n",
    "                if board.game_won():\n",
    "                    # the player who last move won!\n",
    "                    if len(child.full_move_list.moves) == 1:\n",
    "                        print('win now')\n",
    "                    child.assign_q(-board.player_to_move, GameState.WON)\n",
    "\n",
    "                elif board.game_drawn():\n",
    "                    child.assign_q(0, GameState.DRAW)\n",
    "\n",
    "                else:\n",
    "                    q_search_nodes.append(child)\n",
    "                    vector, player = board.get_matrix(as_player=board.player_to_move), board.player_to_move\n",
    "                    q_search_vectors.append(vector)\n",
    "                    q_search_player.append(player)\n",
    "\n",
    "                # unmove for child\n",
    "                board.unmove()\n",
    "\n",
    "            # update p\n",
    "            vector, player = board.get_matrix(as_player=board.player_to_move), board.player_to_move\n",
    "            p_search_vectors.append(vector)\n",
    "            p_search_players.append(player)\n",
    "\n",
    "            # unwind parent\n",
    "            for i in range(len(parent.full_move_list)):\n",
    "                board.unmove()\n",
    "\n",
    "        assert(np.array_equal(board.matrix, validation_matrix))\n",
    "\n",
    "        return q_search_nodes, q_search_vectors, q_search_player, p_search_vectors, p_search_players\n",
    "\n",
    "    def pvs_batch(self, board, nodes_to_expand):\n",
    "\n",
    "        for parent in nodes_to_expand:\n",
    "            for move in parent.full_move_list.moves:\n",
    "                board.hypothetical_move(move[0], move[1])\n",
    "\n",
    "            if board.game_won():\n",
    "                print('game over??!')\n",
    "\n",
    "            for move in parent.full_move_list.moves:\n",
    "                board.unmove()\n",
    "\n",
    "        q_search_nodes, q_search_vectors, q_search_player, p_search_vectors, p_search_players = \\\n",
    "            self.pvs_vectors(board, nodes_to_expand)\n",
    "\n",
    "        if len(q_search_vectors) == 0:\n",
    "            # recalculate for wins/draws\n",
    "            for parent in nodes_to_expand:\n",
    "                parent.recalculate_q()\n",
    "            return False\n",
    "\n",
    "        q_board_vectors = np.array(q_search_vectors).reshape(len(q_search_vectors), self.size, self.size, 1)\n",
    "        p_board_vectors = np.array(p_search_vectors).reshape(len(p_search_vectors), self.size, self.size, 1)\n",
    "\n",
    "        # this helps parallelize\n",
    "        # multiplication is needed to flip the Q (adjust perspective)\n",
    "        q_predictions = np.clip(\n",
    "                    self.value_est.predict([q_board_vectors, np.array(q_search_player)],\n",
    "                                                        batch_size=64).reshape(len(q_search_vectors)),\n",
    "                    a_max=PVSNode.MAX_Q - 0.01,\n",
    "                    a_min=PVSNode.MIN_Q + 0.01\n",
    "            )\n",
    "\n",
    "        log_p_predictions = np.log(self.policy_est.predict([p_board_vectors, np.array(p_search_players)],\n",
    "                                                        batch_size=64).reshape((len(p_search_vectors), self.size ** 2)))\n",
    "\n",
    "        for i, parent in enumerate(nodes_to_expand):\n",
    "            for move in parent.children.keys():\n",
    "                move_index = self.move_to_index(move)\n",
    "                parent.children[move[0], move[1]].assign_p(log_p_predictions[i][move_index])\n",
    "\n",
    "        for i, leaf in enumerate(q_search_nodes):\n",
    "            # update with newly computed q's (only an assignment since approx, we'll compute minimax q's later)\n",
    "            leaf.assign_q(q_predictions[i], GameState.NOT_OVER)\n",
    "\n",
    "        # for all the nodes whose leaves' q's are calculated\n",
    "        for parent in nodes_to_expand:\n",
    "            parent.recalculate_q()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def q(self, board, as_player):\n",
    "        prediction = self.value_est.predict([np.array([board.get_matrix(as_player).reshape(board.size, board.size, -1)]), np.array([as_player])])[0][0]\n",
    "        return prediction\n",
    "\n",
    "    # with epsilon probability will select random move\n",
    "    # returns whether game has concluded or not\n",
    "    def make_move(self, board, as_player, retrain=True, verbose=True, epsilon=0.1, max_depth=5, max_iters=10, k=25):\n",
    "        current_q = self.q(board, as_player)\n",
    "        assert(as_player == board.player_to_move)\n",
    "\n",
    "        move, best_q = self.pvs(board,\n",
    "                                epsilon=epsilon,\n",
    "                                verbose=verbose,\n",
    "                                max_depth=max_depth,\n",
    "                                max_iters=max_iters,\n",
    "                                k=k)\n",
    "\n",
    "        new_q = (1 - self.alpha) * current_q + self.alpha * best_q\n",
    "        print(current_q, best_q)\n",
    "        self.add_train_example(board, as_player, new_q, move)\n",
    "\n",
    "        board.make_move(move[0], move[1])\n",
    "\n",
    "        if board.game_over():\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def one_hot_p(self, move_index):\n",
    "        vector = np.zeros((self.size ** 2))\n",
    "        vector[move_index] = 1.\n",
    "        return vector\n",
    "\n",
    "    def move_to_index(self, move):\n",
    "        return move[0] * self.size + move[1]\n",
    "\n",
    "    def index_to_move(self, index):\n",
    "        return np.int(index / self.size), index % self.size\n",
    "\n",
    "    # adds rotations\n",
    "    def add_train_example(self, board, as_player, result, move, invert_board=False):\n",
    "        board_vectors = board.get_rotated_matrices(as_player=as_player)\n",
    "\n",
    "        for i, vector in enumerate(board_vectors):\n",
    "            clamped_result = max(min(result, MAX_Q), MIN_Q)\n",
    "            self.train_vectors.append((vector, as_player))\n",
    "            self.train_q.append(clamped_result)\n",
    "            # get the i'th rotation\n",
    "            which_rotation = board.get_rotated_point(self.move_to_index(move))[i]\n",
    "            self.train_p.append(self.one_hot_p(which_rotation))\n",
    "            #self.train_p.append(which_rotation)\n",
    "\n",
    "    def update_model(self):\n",
    "\n",
    "        train_inputs = [[], []]\n",
    "        for vector, whose_move in self.train_vectors:\n",
    "            train_inputs[0].append(vector.reshape(self.size, self.size, 1))\n",
    "            train_inputs[1].append(whose_move)\n",
    "            \n",
    "        train_inputs[0] = np.array(train_inputs[0])\n",
    "        train_inputs[1] = np.array(train_inputs[1])\n",
    "\n",
    "        print(len(self.train_vectors))\n",
    "        if len(self.train_vectors) > 0:\n",
    "            self.value_est.fit(x=train_inputs,\n",
    "                                y=np.array(self.train_q),\n",
    "                                shuffle=True,\n",
    "                                validation_split=0.1)\n",
    "            self.policy_est.fit(x=train_inputs,\n",
    "                                y=np.array(self.train_p),\n",
    "                                shuffle=True,\n",
    "                                validation_split=0.1)\n",
    "\n",
    "        max_vectors = 100000\n",
    "        while len(self.train_vectors) > max_vectors:\n",
    "            self.train_vectors = self.train_vectors[100:]\n",
    "            self.train_p = self.train_p[100:]\n",
    "            self.train_q = self.train_q[100:]\n",
    "\n",
    "        print('Num Train Vectors', len(self.train_vectors))\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.value_est.save(filename + '_value.net')\n",
    "        self.policy_est.save(filename + '_policy.net')\n",
    "\n",
    "    def load_net(self, filename):\n",
    "        self.value_est = keras.models.load_model(filename + '_value.net')\n",
    "        self.policy_est = keras.models.load_model(filename + '_policy.net')\n",
    "       \n",
    "    def load(self, value_file, policy_file):\n",
    "        self.value_est = keras.models.load_model(value_file)\n",
    "        self.policy_est = keras.models.load_model(policy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def play_a_game():\n",
    "    mind = PQMind(size=SIZE, alpha=0.2, turn_input=True, init=False)\n",
    "    mind.value_est.set_weights(q_model_bc.value)\n",
    "    mind.policy_est.set_weights(p_model_bc.value)\n",
    "    round_board = Board(size=SIZE, win_chain_length=WIN_CHAIN_LENGTH)\n",
    "    \n",
    "    # randomize the board a bit\n",
    "    for i in range(random.randint(0, 3)):\n",
    "        round_board.make_random_move()\n",
    "    \n",
    "    current_player = round_board.player_to_move\n",
    "    while True:\n",
    "        result = mind.make_move(round_board,\n",
    "                                as_player=current_player,\n",
    "                                retrain=True,\n",
    "                                epsilon=0.1,\n",
    "                                max_depth=25,\n",
    "                                k=SIZE ** 2,\n",
    "                                max_iters=50,\n",
    "                                )\n",
    "        print(round_board.pprint())\n",
    "        current_player = -current_player\n",
    "        if result:\n",
    "            break\n",
    "            \n",
    "    return mind.train_vectors, mind.train_p, mind.train_q\n",
    "\n",
    "SIZE = 7\n",
    "WIN_CHAIN_LENGTH = 5\n",
    "\n",
    "mind = PQMind(size=SIZE, alpha=0.2, turn_input=True, init=True)\n",
    "q_model = mind.value_est\n",
    "p_model = mind.policy_est\n",
    "\n",
    "def distributed_play():\n",
    "\n",
    "    collected = sc.parallelize(range(400)).repartition(400).map(lambda x : play_a_game()).collect()\n",
    "\n",
    "    train_vectors = []\n",
    "    train_p = []\n",
    "    train_q = []\n",
    "\n",
    "    for vector, p, q in collected:\n",
    "        train_vectors.extend(vector)\n",
    "        train_p.extend(p)\n",
    "        train_q.extend(q)\n",
    "\n",
    "\n",
    "    train_inputs = [[], []]\n",
    "    for vector, whose_move in train_vectors:\n",
    "        train_inputs[0].append(vector.reshape(SIZE, SIZE, 1))\n",
    "        train_inputs[1].append(whose_move)\n",
    "\n",
    "    train_inputs[0] = np.array(train_inputs[0])\n",
    "    train_inputs[1] = np.array(train_inputs[1])\n",
    "\n",
    "    if len(train_vectors) > 0:\n",
    "        q_model.fit(x=train_inputs,\n",
    "                            y=np.array(train_q),\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.1)\n",
    "        p_model.fit(x=train_inputs,\n",
    "                            y=np.array(train_p),\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "    print('Num Train Vectors', len(train_vectors))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "for i in range(epochs):\n",
    "    q_model_bc = sc.broadcast(copy.deepcopy(q_model.get_weights()))\n",
    "    p_model_bc = sc.broadcast(copy.deepcopy(p_model.get_weights()))\n",
    "    distributed_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind.value_est.set_weights(q_model.get_weights())\n",
    "mind.policy_est.set_weights(p_model.get_weights())\n",
    "mind.save('models/distributed_5_bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [hseokho.q-gomoku]",
   "language": "python",
   "name": "hseokho.q-gomoku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}