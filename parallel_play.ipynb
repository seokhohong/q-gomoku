{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Input, Convolution2D, Dense, Dropout, Flatten, concatenate, BatchNormalization\n",
    "from keras.models import Model  # basic class for specifying and training a neural network\n",
    "from keras import losses\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "import gomoku\n",
    "from gomoku.learner.pqmind import PQMind\n",
    "from gomoku.core.board import Board\n",
    "import random\n",
    "\n",
    "random_state = np.random.RandomState(42)\n",
    "\n",
    "MIN_Q = -1\n",
    "MAX_Q = 1\n",
    "\n",
    "import importlib\n",
    "importlib.reload(gomoku.learner.pqmind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_iter_function(i):\n",
    "    if i == 0:\n",
    "        return 1\n",
    "    elif i < 3:\n",
    "        return 10\n",
    "    return 20\n",
    "\n",
    "def play_a_game(i):\n",
    "    mind = PQMind(size=SIZE, alpha=0.2, turn_input=True, init=False)\n",
    "    mind.value_est.set_weights(q_model_bc.value)\n",
    "    mind.policy_est.set_weights(p_model_bc.value)\n",
    "    round_board = Board(size=SIZE, win_chain_length=WIN_CHAIN_LENGTH)\n",
    "    \n",
    "    # randomize the board a bit\n",
    "    for i in range(random.randint(0, 3)):\n",
    "        round_board.make_random_move()\n",
    "    \n",
    "    current_player = round_board.player_to_move\n",
    "    while True:\n",
    "        result = mind.make_move(round_board,\n",
    "                                as_player=current_player,\n",
    "                                retrain=True,\n",
    "                                epsilon=0.1,\n",
    "                                max_depth=25,\n",
    "                                k=SIZE ** 2,\n",
    "                                max_iters=max_iter_function(i),\n",
    "                                )\n",
    "        print(round_board.pprint())\n",
    "        current_player = -current_player\n",
    "        if result:\n",
    "            break\n",
    "            \n",
    "    return mind.train_vectors, mind.train_p, mind.train_q\n",
    "\n",
    "SIZE = 7\n",
    "WIN_CHAIN_LENGTH = 5\n",
    "\n",
    "mind = PQMind(size=SIZE, alpha=0.2, turn_input=True, init=True)\n",
    "q_model = mind.value_est\n",
    "p_model = mind.policy_est\n",
    "\n",
    "def distributed_play(i):\n",
    "\n",
    "    collected = sc.parallelize(zip(range(400), range(400))).partitionBy(400, lambda x: x) \\\n",
    "                    .map(lambda x : play_a_game(i)).collect()\n",
    "\n",
    "    train_vectors = []\n",
    "    train_p = []\n",
    "    train_q = []\n",
    "\n",
    "    for vector, p, q in collected:\n",
    "        train_vectors.extend(vector)\n",
    "        train_p.extend(p)\n",
    "        train_q.extend(q)\n",
    "\n",
    "\n",
    "    train_inputs = [[], []]\n",
    "    for vector, whose_move in train_vectors:\n",
    "        train_inputs[0].append(vector.reshape(SIZE, SIZE, 1))\n",
    "        train_inputs[1].append(whose_move)\n",
    "\n",
    "    train_inputs[0] = np.array(train_inputs[0])\n",
    "    train_inputs[1] = np.array(train_inputs[1])\n",
    "\n",
    "    if len(train_vectors) > 0:\n",
    "        q_model.fit(x=train_inputs,\n",
    "                            y=np.array(train_q),\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.1)\n",
    "        p_model.fit(x=train_inputs,\n",
    "                            y=np.array(train_p),\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "    print('Num Train Vectors', len(train_vectors))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "for i in range(epochs):\n",
    "    q_model_bc = sc.broadcast(copy.deepcopy(q_model.get_weights()))\n",
    "    p_model_bc = sc.broadcast(copy.deepcopy(p_model.get_weights()))\n",
    "    distributed_play(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind.value_est.set_weights(q_model.get_weights())\n",
    "mind.policy_est.set_weights(p_model.get_weights())\n",
    "mind.save('models/distributed_5_bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [hseokho.q-gomoku]",
   "language": "python",
   "name": "hseokho.q-gomoku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}